\newcommand{\keyword}[1]{\textbf{#1}}

[It is incredibly important we cite \cite{thebook} properly throughout here: it has directed us to many good papers and starting points, and influenced the organization of this section heavily.]

The R-tree was introduced with the search, or \keyword{range} query.
Soon it was being applied to a variety of other queries.
Its apparent universality for spatial queries is natural motivation for improving R-tree performance.
In this section we discuss various advances beyond Guttman's original design.
Along the way we hope to imply some intuition about the underlying theory for R-tree performance.
In the next section we show what formal theory and considerations for R-trees have so far been developed.

Before delving into the improvements for R-trees it is important to provide some impression of the broad context in which we find R-trees.
Namely, what are the spatial queries R-trees are used for?
How are these queries efficiently satisfied using the structure of the R-tree?
As it happens there is an inordinate number of queries for which R-trees may be used, and we cannot hope to catalog them all here.
One broad classification scheme groups queries into three types.
\keyword{Topological} queries are based on basic geometric properties of the data.
The quintessential \keyword{range} query falls into this category.
\keyword{Directional} queries involve predicates of the form ``above'', or ``east''---filtering according to global positions.
An example of this is skyline, or dominating points computations.
Lastly, \keyword{distance} queries consider distance between data elements.
The exemplar here is nearest-neighbor computation.
Within these three categories there exists a great many queries.
Even accounting for the different types of topological relations between MBRs is a daunting task \cite{papadiassellistheodoridisegenhofer95}.
Also note that these categories are not a neat partition: would the ``nearest surrounder'' query be directional or distance?

In light of this overwhelming diversity we will consider just three queries and walk through their execution on an R-tree.
The goals of this in-depth examination are three:
provide a context for the reader to keep in mind as we discuss R-tree improvements;
demonstrate how R-trees can service very different queries;
demonstrate how---despite their differences---these varied queries share similar bottlenecks in the R-tree, implying general optimizations are possible.
The three queries are \keyword{range}, \keyword{spatial join}, and \keyword{nearest neighbor}.

\subsection{The Range Query}
The range query is perhaps the simplest spatial query and remains essentially unchanged since its introduction \cite{guttman84}.
The idea is this: given a database of $R$ spatial objects represented by their MBRs and an input rectangle $s$, output all $r\in R$ such that $r\cap s\neq\emptyset$.
Described briefly in English: at sub-tree $T$, recurse at each child if that child's MBR has a nonempty intersection with $s$.
If $T$ is a leaf, output $T$ if it has a nonempty intersection with $s$.
There exist many variations---perhaps the query is only for objects strictly containing, or contained within, $s$ \cite{gaedegunther98}.
This general query's popularity has warranted intense study, and extensive tests have compared how R-tree variants perform on different inputs \cite{papadiassellistheodoridisegenhofer95}.

We take the time here to emphasize that even in this fundamental query there exist complications beyond just the various forms the query can take.
Some thought reveals that a range query can result in output equal to $\emptyset$ or to the whole tree.
Moreover, this is true even if the range is the degenerate case: the point.
A surprising fact is that there exist R-trees and queries such that the output is $\emptyset$ but the \emph{whole tree must be traversed}.
This is true not just for Guttman's initial R-tree, but many advanced types as well (see Theorem 3 of \cite{argeberghaverkortyi04}).
As testament to its utility, we note that the range query on R-trees was adapted to \emph{approximating similarities between frequencies} \cite{agrawalfaloutsosswami93} by mapping the $k$ major features of the DFT transform into $k$-dimensional space.

\subsection{The Spatial Join}
A spatial join is nothing more than a general join, just with a spatial predicate---namely, join $r_1$ with $r_2$ iff $r_1\cap r_2\neq\emptyset$ \cite{brinkhoffkriegelseeger93}.
This is a more general operation than range query---that is, all variations of range queries are easily reducible to the appropriate variant of a spatial join \cite{gaedegunther98}.
The core spatial join algorithm \cite{brinkhoffkriegelseeger93} follows the same principles as the range query.
It assumes that the two trees, $R_1$ and $R_2$, are the same height.
For a given child $n_1\in R_1$ and child $n_2\in R_2$, if $n_1\cap n_2=\emptyset$ it then moves on to the next child in $R_2$.
Otherwise, we recurse in a DFS fashion.
Observe that if the MBR are better-clustered in some fashion, we can conclude that $n_1\cap n_2=\emptyset$ at a higher level in the tree, thus saving page accesses.
There exists a variation of spatial join over R-trees which proceeds in a breadth-first-search fashion \cite{huangjingrundensteiner97}.
This is motivated by the idea that understanding how the join behaves at level $k$ might better guide the iteration over the pages in level $k+1$.
The underlying motivation for this alternative is that it is a hard problem to figure out which nodes we ``really'' want to explore, given only the MBRs.

See also \cite{papadopoulosrigauxscholl99} for evaluating different spatial joins on varios R-tree variants.
See also \cite{vassilakopouloscorralkaranikolas11} for join within a single R-tree, self-joins.
Find a paper (there are many) that say join reasoning from B-trees don't naturally extend to R-trees.
Huge survey \cite{jacoxsamet07}.

\subsection{The Nearest Neighbor Query}
The nearest neighbor (NN) query takes as input a point $p$ and outputs the $k$ nearest objects to it (usually under Euclidean distance) \cite{roussopouloskelleyvincent95}.
The algorithm for NN on R-trees differs from range or join in part because it is not merely doing set-theoretic operations on the data. 
Rather, at each level of the tree we compute MINDIST, the minimum distance from $p$ to \emph{some} object in the MBR $R$ for each $R$ at that level.
This naturally leads to a DFS traversal: at level $k$ we computer the MINDIST to each node, sort them in that order, and recurse on the first one.
Some reasoning about the properties of MINDIST allows for safe pruning of these lists, allowing us to skip many nodes.
The heuristic of sorting by MINDIST is not optimal---rather, it is ``optimistic'' in the sense that if the MBRs really do reflect the nearest point, it will terminate quite quickly.
There exist other heuristics as well in the same paper \cite{roussopouloskelleyvincent95}.

See also \cite{corralalmendros-jimenez07}, comparisons between R-tree performance on various distance queries.

\subsection{Optimization Intuition}
From these three queries, the underlying cause of inefficiency is obvious: that for any non-leaf node, the MBR does not give us all the information we need to know if we should explore it or not.
If we are wasteful during a range query, it is because our inner node MBRs are so big-but-sparse that we have to search them despite their children's empty intersection.
We can be wasteful during a join for much the same reason.
In a nearest neighbor search, if the inner MBRs do not provide a good approximation of their children, then the MINDIST values would fail to allow much pruning in our branch-and-bound.
Informally speaking, it seems that all these queries suffer if the inner MBRs cover much more area than their children, or the inner MBRs have a much greater degree of overlap with each other than the actual data (leaves).
So in our efforts to improve our R-tree performance, we want to somehow grow a ``better-organized tree''.
Implicit in this desire is an important difference between B-trees and R-trees.
Consider a set of data $\mathcal S$, an index $I$ implying an order-of-insertion for those elements $s\in \mathcal S$, and the B-tree $B_I$, generated by inserting $s_{i+1}$ into $B_I$ after $s_{i}$.
Whatever $I$ we choose, by the linear nature of $\mathcal S$, the resulting $B_I$ will not differ ``too much'' from any $B_{I\prime}$.
Obviously, the leaves always end up in the same order.
However, this is \emph{not} the case at all for R-trees.
Given $I$ and $I\prime$, the R-trees $R_I$ and $R_{I\prime}$ may wildly differ in terms of the inner nodes created.
Thus our idea of growing an R-tree in a ``smarter'' fashion has real meaning.

\paragraph{Node Splitting}
As a basic R-tree's construction (and hence performance) is somewhat at the mercy of its insertion order, a natural first step to improving R-trees is to take the time to split nodes so that the tree maintains a good structure.
When faced with the task of splitting a node in a B-tree, the split is obvious.
When splitting an R-tree node, however, we have the freedom and burden to choose among $\binom{M}{m}$ possible partitions of $M$ data points into 2 sets of size $m=\frac{M}{2}$.
In conceiving the R-tree Guttman realized the importance of a ``good'' split and proposed three algorithms for it.
The initial \cite{guttman84} followed by the heuristic improvement in R* \cite{beckmannkriegelschneiderseeger90} followed by optimal \cite{garcialopezleutenegger98}.
This isn't the end-all be-all, as 

More persistent tree improvement (how dose this mesh with ``applying static advancements''?).
See \cite{leehsujensencuiteo03} for more tree reorg.

Simplifying splits: have a strict ordering, see \cite{kamelfaloutsos94}.


\paragraph{Static R-Tree Construction}
In our concern for computing a good split as the R-tree recieves new data, we are naturally tempted to imagine the situation where the data is unchanging, static.
This is far from contrived: the 1930s US census data is unlikely to change.
The paper on bulk loading \cite{garcialopezleutenegger98a}.
Index optimization \cite{gavrila94}.
Hilbert R-tree \cite{kamelfaloutsos94}.

\paragraph{Optimizing Dynamic Space Usage}
In our considering static R-trees, we have seen the utility of dense packing.
Intuitively, if the tree is dense, there are fewer nodes (perhaps even fewer layers), and so we have less tree to check.
This gives us motivation for improving space usage in dynamic R-trees: not to \%100 of course, but in read-heavy loads B-trees benefit from more density, so it is natural to be curious if we can make the same improvements for R-trees.
Improving spatial usage, dense R-trees.
The only citation from \cite{thebook} is \cite{huanglinlin01}.
I bet more have arrived.
Dynamic Hilbert R-tree from \cite{kamelfaloutsos94}, the classic.

\paragraph{Applying Static Advancements to Dynamic R-Trees}
Grafting is a cool advance, but seemingly ignored \cite{schrekchen00}.
Observe that R* has some satic improvements \cite{beckmannkriegelschneiderseeger90}.
Is there a quintessential bulk loading-in-dynamic trees or lazy paper?
Perhaps this covers both, to start with: \cite{argehinrichsvahrenholdvitter99}.
This introduces the LUR, the lazy update R-tree \cite{kwonleelee02}.

\paragraph{Avoiding Overlapping MBRs}
A fundamental inefficiency with the R-tree---especially for range queries---is that we must sometimes guess which subtree on which to recurse, thereby often computing ``dead-ends''.
This guessing occurs when our query overlaps the MBR of our R-tree.
This is unavoidable, but the likelihood of this query overlap is minimized when the overlap among our MBRs is minimized.
This motivates the definition of the \emph{stabbing number} \cite{berggudmundssonhammarovermars00}.
Consider the MBRs of a single layer of the R-tree (the leaves constitute a layer).
The stabbing number is the maximum number of rectangles containing a single point---querying that point ``stabs'' the greatest number of rectangles possible.
The authors \cite{berggudmundssonhammarovermars00} use this notion to develop some formal lower bounds on R-tree queries.

Since its conception, substantive work has gone into determining the feasibility of computing optimal partitionings---i.e., MBRs with minimal stabbing number.
We can formalize our desire as computing a $r$-partition of our $N$ rectangles such that our $r$ new MBRs have \emph{minimal stabbing number}.
As it turns out, computing such a partition is $\NP$-hard; on the other hand, it is parameterized by $k$, which quickly becomes small as we consider each layer of the R-tree \cite{bergkhosraviverdonschotweele11}.
The algorithm was ultimately considered too slow, but perhaps future work will make it practical.
[Other work also exists on this, modern stuff too.]

Prior to this formal approach, the R+ tree \cite{sellisroussopoulosfaloutsos87} was the first to attempt to minimize stabbing number.
They changed the R-tree's design such that no MBRs on a single, inner layer overlapped with their siblings.
(Obviously we can only control the inner layers---the leaves may overlap.)
This design substantively deviates from ``normal'' R-tree implementations in that leaves are replicated, if they are contained in the MBR of multiple inner nodes.
This facilitates fast \emph{range} queries, but complicates other queries \cite{some paper talks about how it breaks down on perimiter queries or something---gaedegunther?samet?}.


\paragraph{Provable Optimality}
Basically, talk briefly about the optimal R-tree.
Or maybe this should be a lead-in to the formal discussion.
