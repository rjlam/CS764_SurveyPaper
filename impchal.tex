\newcommand{\keyword}[1]{\textbf{#1}}


The \rstar-tree is the de facto standard R-tree implementation \cite{tocite}.
Its success is due to the effective heuristics used to improve the R-tree performance---somehow its algorithms produce ``better'' R-trees \cite{tocite}.
The notion of an R-tree being better or worse than another is hard to define \cite{tocite}.
For instance, lacking a strict ordering, spatial data does not lend itself to a ``natural'' optimal structure \cite{gaedegunther98}.
Moreover, the sheer variety of spatial queries makes it difficult to understand what operations we're actually trying to improve.

In this section we will explore and list many improvements and changes made to R-trees from a conceptual perspective.
The recent book on R-trees \cite{thebook} has a comprehensive list up to its publication on the advancements made to R-trees.
Here we attempt to illustrate more the common patterns emerging from these results.
In order to motivate and provide context for this presentation, we first introduce and discuss some exemplar queries.
After this we present the major themes towards improving \rbase-trees.
In the next section we work towards a theory of \rbase-trees, and how we can understand their behavior.

\subsection{Spatial Queries}
The original query for which R-trees were designed was the ``search'' query \cite{guttman84}, now generally called the \emph{range} query \cite{tocite}.
Briefly, given a range ``window'' (simply an axis-aligned rectangle, much like the MBRs in the R-tree itself) is provided as input, and the result is all leaves which have a nonempty intersection.
However, the R-tree soon found itself applied to many, many spatial queries---testament to its success \cite{tocite}.
We simply cannot catalog all of the queries to which R-trees have been applied, but we will discuss some main examples.
To motivate our selection, we can loosely classify spatial queries into \emph{topological}, \emph{directional}, \emph{distance}, and \emph{join} queries \cite{thebook,manolopoulos2003r}.

Topological queries depend on the topological relationship between the data and our query.
Range is exactly a topological query, but even then there are many subvarieties \cite{papadiassellistheodoridisegenhofer95,gaedegunther98}.
Directional queries imply a direction the data (e.g., the cardinal directions on geographic data) and the query depends on that global characteristic.
A skyline query \cite{papadiastaofuseeger05} is an example of such a query.
Distance queries assume (almost always Euclidean) distance between the spatial data and asks related queries.
The fundamental example of this sort of query is the nearest-neighbor query \cite{roussopouloskelleyvincent95}.
Lastly, spatial join is nothing more than a join query with a spatial predicate \cite{brinkhoffkriegelseeger93} such as ``intersects''.
Thus, many of the previous query types can be reduced to a spatial join \cite{tocite}, but it is useful to consider these separate issues.
Before examining exemplar queries in-depth, we emphasize that this taxonomy is not a strict partition---would the \emph{nearest surrounder} query \cite{leeleeleong10} be a directional or distance query?
These are really just to give a sense of what R-trees are used for, not everything they can do.

The three queries we will consider in depth are \emph{range}, \emph{spatial join}, and \emph{nearest neighbor}.
These three are generally accepted as the key queries to consider when thinking about R-trees.
In examining these queries we will find that, despite their differences, they all suffer the same fundamental bottlenecks, implying general improvements for R-trees are indeed possible.
We will also briefly mention \emph{aggregate queries}, more complex spatial operations which are built on a foundation of R-trees, but are higher-level than our exemplars.

\paragraph{The Range Query}
The range query is perhaps the simplest spatial query and remains essentially unchanged since its introduction \cite{guttman84}.
The idea is this: given a database of $R$ spatial objects represented by their MBRs and an input rectangle $s$, output all $r\in R$ such that $r\cap s\neq\emptyset$.
Described briefly in English: at sub-tree $T$, recurse at each child if that child's MBR has a nonempty intersection with $s$.
If $T$ is a leaf, output $T$ if it has a nonempty intersection with $s$.
There exist many variations---perhaps the query is only for objects strictly containing, or contained within, $s$ \cite{gaedegunther98}.
This general query's popularity has warranted intense study, and extensive tests have compared how R-tree variants perform on different inputs \cite{papadiassellistheodoridisegenhofer95}.

We take the time here to emphasize that even in this fundamental query there exist complications beyond just the various forms the query can take.
Some thought reveals that a range query can result in output equal to $\emptyset$ or to the whole tree.
Moreover, this is true even if the range is the degenerate case: the point.
A surprising fact is that there exist R-trees and queries such that the output is $\emptyset$ but the \emph{whole tree must be traversed}.
This is true not just for Guttman's initial R-tree, but many advanced types as well (see Theorem 3 of \cite{argeberghaverkortyi04}).
As testament to its utility, we note that the range query on R-trees was adapted to \emph{approximating similarities between frequencies} \cite{agrawalfaloutsosswami93} by mapping the $k$ major features of the DFT transform into $k$-dimensional space. [Find more applications?]

\paragraph{The Spatial Join}
A spatial join is nothing more than a general join, just with a spatial predicate---namely, join $r_1$ with $r_2$ iff $r_1\cap r_2\neq\emptyset$ \cite{brinkhoffkriegelseeger93}.
This is a more general operation than range query---that is, range queries are easily reducible to the appropriate variant of a spatial join \cite{gaedegunther98}.
The core spatial join algorithm \cite{brinkhoffkriegelseeger93} follows the same principles as the range query.
It assumes that the two trees, $R_1$ and $R_2$, are the same height.
For a given child $n_1\in R_1$ and child $n_2\in R_2$, if $n_1\cap n_2=\emptyset$ it then moves on to the next child in $R_2$.
Otherwise, we recurse in a DFS fashion.
Observe that if the MBR are better-clustered in some fashion, we can conclude that $n_1\cap n_2=\emptyset$ at a higher level in the tree, thus saving page accesses.
Dealing with R-trees of varying height is a nontrivial, but natural extension.

There exists a variation of spatial join over R-trees which proceeds in a breadth-first-search fashion \cite{huangjingrundensteiner97}.
This is motivated by the idea that understanding how the join behaves at level $k$ might better guide the iteration over the pages in level $k+1$.
The underlying motivation for this alternative is that it is a hard problem to figure out which nodes we ``really'' want to explore, given only the MBRs.
[Other variations.]
Of course, there exist experimental comparisons between these variations \cite{papadopoulosrigauxscholl99}, and at the time they concluded buffer size was the main focus point.

A more modern survey \cite{jacoxsamet07} has a thorough accounting of more recent approaches to spatial join.
[What's a nice summary it can give.]

There are still more modern variations of spatial join being considered, see \cite{vassilakopouloscorralkaranikolas11} for join within a single R-tree, self-joins.
Huge survey \cite{jacoxsamet07}.

\paragraph{The Nearest Neighbor Query}
The nearest neighbor (NN) query takes as input a point $p$ and outputs the $k$ nearest objects to it (usually under Euclidean distance) \cite{roussopouloskelleyvincent95}.
The algorithm for NN on R-trees differs from range or join in part because it is not merely doing set-theoretic operations on the data. 
Rather, at each level of the tree we compute MINDIST, the minimum distance from $p$ to \emph{some} object in the MBR $R$ for each $R$ at that level.
This naturally leads to a DFS traversal: at level $k$ we computer the MINDIST to each node, sort them in that order, and recurse on the first one.
Some reasoning about the properties of MINDIST allows for safe pruning of these lists, allowing us to skip many nodes.
The heuristic of sorting by MINDIST is not optimal---rather, it is ``optimistic'' in the sense that if the MBRs really do reflect the nearest point, it will terminate quite quickly.
There exist other heuristics as well in the same paper \cite{roussopouloskelleyvincent95}.

The NN query is the basic distance query for other distance queries such as spatial aggregate queries (perimeter, average-area, and so on) \cite{corralalmendros-jimenez07}.
There are more complex variations of NN, such an the \emph{aggregate} NN query.
There the objective is to find a point $p$ such that its $k$ nearest neighbors (for fixed $k$) have minimial \emph{total} distance.
Exact optimal is for aggregate nearest neighbors is \cite{papadiastaomouratidishui05}, according to \cite{liliyiyaowang11}.
In \cite{liliyiyaowang11} NN is further generalized to \emph{flexible aggregate similarity search}, where a more abstract notion of distance and relations can be used.
In that paper they review much of the history of NN, of which R-trees played an important part.
They also emphasize one real weakness of R-trees: their performance does fail for large dimensional data ($>6$, in this case).

\paragraph{Aggregate queries}
As a glimpse of what's ``beyond'' NN queries, we briefly mention two more aggregate queries: \emph{convex hull} and then \emph{skyline}.
There are well-known algorithms for convex hull in a non-database setting \cite{clrs3rd}, but adapting for huge data takes real work.
Bohm and Kriegel \cite{bohmkriegel01} did so, relying on multidimensional indices (namely, R-trees).
This has been the foundation of important subroutines, including many of the NN queries mentioned as well as, for instance, \emph{Furthest} Neighbors \cite{yaolikumar09}.

The skyline operator was introduced for databases in \cite{borzsonyikossmanstocker01}.
Intuitively, it seeks to find all points that are ``undominated'' in a dimension.
The example in that original paper is to find the best, cheapest hotel: thus the skyline gives us all hotels that are better than all other hotels of greater price, or (equivalently) cheaper than all other hotels of the same or better quality.
Where all the previous queries have been presented as something like ends in and of themselves, the skyline operator (as the name implies) is assumed to be used in conjunction with other operators.
A paper soon followed to implement \emph{on-line} skyline computation primarily using the NN query\cite{kossmanramsakrost02}.
They reasoned that the NN query is best implemented via the \rstar-tree, for their purposes.
Skyline computation remains an active area of research---\cite{zhangalhajj09} has a more thorough history and state of the art than presented here.

The take-away of discussing these ``extra queries'' is that R-trees seem to be a strong building block for even very complex spatial queries.

\subsection{Optimization Intuition}

From these queries we start to suspect the underlying cause of inefficiency is that, for any non-leaf node, the MBR does not give us all the information we need to know if we should explore it or not.
If we are wasteful during a range query, it is because our inner node MBRs are so big-but-sparse that we have to search them despite their children's empty intersection.
We can be wasteful during a join for much the same reason.
In a nearest neighbor search, if the inner MBRs do not provide a good approximation of their children, then the MINDIST values would fail to allow much pruning in our branch-and-bound.
[Add section about aggregate queries, skyline bottlenecks.]

Informally speaking, it seems that all these queries suffer if the inner MBRs cover much more area than their children, or the inner MBRs have a much greater degree of overlap with each other than the actual data (leaves).
So in our efforts to improve our R-tree performance, we want to somehow grow a ``better-organized tree''.

 % Implicit in this desire is an important difference between B-trees and R-trees.
 % Consider a set of data $\mathcal S$, an index $I$ implying an order-of-insertion for those elements $s\in \mathcal S$, and the B-tree $B_I$, generated by inserting $s_{i+1}$ into $B_I$ after $s_{i}$.
 % Whatever $I$ we choose, by the linear nature of $\mathcal S$, the resulting $B_I$ will not differ ``too much'' from any $B_{I^\prime}$.
 % Obviously, the leaves always end up in the same order.
 % However, this is \emph{not} the case at all for R-trees.
 % Given $I$ and $I^\prime$, the R-trees $R_I$ and $R_{I^\prime}$ may wildly differ in terms of the inner nodes created.
 % Thus our idea of growing an R-tree in a ``smarter'' fashion has real meaning.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Improving Tree-Shape (Reducing Overlap)}
Perhaps the most common guiding principle for growing R-trees efficiently is to reduce the overlap between MBRs.
In our discussion we consider only range queries---this is both one of the few queries for which we have a strong understanding, and is fundamental enough to reasonably hope its efficiency influences more complex queries.

Note that, as MBRs on different levels are either wholly contained or entirely disjoint from one another, when we discuss overlap between MBRs we assume they are on the same level of the R-tree.
The only exception, of course, are the MBRs for the leaves.
We also, for shorthand, refer to nodes as their own MBRs, making it natural to speak about the ``intersection of two nodes'' (by which we mean the intersection of their MBRs).
Guttman \cite{guttman84} presumed that minimizing the total area was the best choice for optimizing search (range) queries.
Thus, on a node split, for example, the ``optimal algorithm'' considers all possible partitions of the children, and chooses that which minimizes the \emph{sum} of the area of the two new children.
In their \rstar-paper \cite{beckmannkriegelschneiderseeger90} Beckmann et.\ al.\ credit a large part of their success to how the \rstar-tree instead tries to minimize \emph{overlap}.
There it is defined as the total area of the pairwise intersections of constituent rectangles.
They also note that determining the best function to minimize---area versus overlap---could not be realistically determined analytically, so they turned to experimental methods.
The \rplus-tree \cite{sellisroussopoulosfaloutsos87} approached the idea of reducing overlap from another direction, and modified the R tree such that \emph{none} of the inner MBRs overlapped.
If an input rectangle stretched across multiple nodes $N_1,N_2$, then both nodes would point to that rectangle, but $N_1\cap N_2=\emptyset$.
A comparison between \rbase-trees and \rplus-trees can be found in \cite{greene89}.
While largely superseded by the \rstar-tree, the \rplus variant joins it as regular ``baseline'' tree against which new developments are compared \cite{something}.

These algorithms have tried to make the \rbase-tree grow more intelligently as insertions happen.
We repeat a key fact stated in \cite{beckmannkriegelschneiderseeger90}: R-tree performance is very much dependent on the order of insertions.
Whereas a B-tree's leaves always end up in the same order regardless of the permutation under which they were inserted, the leaves of an R-tree can be in wildly different locations \cite{tocite,tocite}.
From this observation \cite{beckmannkriegelschneiderseeger90} introduced the notion of forced-reinsertion.
If a node is about to split, they first see if its constituents can be re-inserted at the same level.
This both saves a split and works to overcome the bias the tree may have from its first insertions.
This idea is extended in \cite{schrekchen00} via \emph{branch grafting}.
It was initially conceived of independent of \rstar-tree force-reinsertion method, but is quite similar.
Motivated by some of the observations of Greene \cite{greene89}, the idea is to graft node $N$'s children to a sibling node, if that would improve the shape of the tree.
In general the idea of improving the shape of the tree by these retrospective modifications seems appealing.
Unfortunately, in modern highly-parallel databases reorganizing a tree is often too difficult, and does bad things to the buffer \cite{beckmannseeger09}.

We have discussed improving R-tree shape mainly under the guiding intuition that reducing \emph{overlap} is our goal.
Can we formalize this?
One suggestion is as follows.
The notion of minimizing overlap was formalized as minimizing the \emph{stabbing number} \cite{berggudmundssonhammarovermars00}.
The stabbing number of a set of shapes $\mathcal S$ is the maximum number of shapes in $\mathcal S$ intersecting a point.
Intuitively, that point ``stabs'' the most number of shapes possible.
This lead to the first nontrivial upper-bounds on R tree operations, albeit in a static---no inserting or deleting elements---setting.
Stabbing numbers have become a subject in their own right, and continue to be motivated by R trees and other spatial structures with (potentially) overlapping rectangles.
It was just recently shown that computing the partition of $\mathcal S$ into $r$ parts in a way such that the partitions' MBRs have minimal stabbing number is $\NP$-hard \cite{bergkhosraviverdonschotweele11}.
This is somewhat ameliorated by their demonstrating that there is an algorithm for computing the partition exponential in $r$, which quickly becomes small in R trees.
However, even their approximation algorithms are not yet computed competitively quickly.
[Mention: Research continues on this topic \cite{durochermehrabi12}, others? From a graphics/geometry perspective.]

\paragraph{Improving Tree-Shape by Delaying Insertions}
We have seen some techniques to keep the tree well-formed during dynamic insertions.
A natural question follows: what if the insertions were not so dynamic?
The idea of bulk-loading to quickly build a static database naturally extends to \rbase-trees, and indeed ultimately lead to the notion of the Hilbert \rbase-trees discussed in the next section \cite{kamelfaloutosos94}.

The paper by Arge et. al. \cite{argehinrichsvahrenholdvitter99} contains an excellent summary of bulk-loading for \rbase-trees, and also extends that idea to bulk-updating a dynamic \rbase-tree.
This idea has been explored by others as well.
\cite{biveinissaltenisjensen07}.

Let us extend the idea of less-dynamic insertions to its logical conclusion: completely static data.
This is quite common is spatial databases: the 1950s census is unlikely to change.
Initially, bulk loading was motivated by space utilization \cite{tocite}, and indeed we shall see that dynamic \rbase-trees have trouble making good use of space \cite{tocite}. % see small R-tree section.
Some of the initial papers were \cite{tocite}.
Bulk loading continues to adapt as the world changes \cite{tanluomaoni12}. 
[to expand].

[We add the brief note that minimizing overlap usually leads to minimizing area of the relevant MBRs.
Something about locality of reference.
Is there a quintessential bulk loading-in-dynamic trees or lazy paper?
To start with: \cite{argehinrichsvahrenholdvitter99}.]

\paragraph{Imposing an Ordering}
Our methods of improving tree-shape follow mainly from intuition and experimental results.
Is there any formal way of dictating \rbase-tree organization other than heuristics?

Recall that much of the complexity of \rbase-trees really follows from the fact that spatial data does not have a strict ordering.
If it did, we could suddenly bring to bear the work from B-trees, and apply our more ready intuition in working with strictly ordered data.
From this motivation sprung the Hilbert \rbase-tree \cite{kamelfaloutsos94}.
It uses a space-filling curve:
for example, in $[0,10]^2\subseteq\mathbb Z^2$, one may conceive of a left-to-right, bottom-to-top walk which first visits $(0,0)$, then $(0,1)$, then \ldots, then $(10,9)$ and $(10,10)$.
So we may characterize a value by when it appears on a walk---generalizing this we get a strict ordering on arbitrary points in $\mathbb Z^2$!

The Hilbert \rbase-tree uses a Hilbert curve for its walk, and extends it to (approximation of) real points on $\mathbb R^n$.
There are other space-filling curves, but it was concluded experimentally that it has good clustering properties \cite{kamelfaloutsos94} over the alternatives.
Moreover, a nice property is that the resulting nodes are usually ``square-like'', which is a rule-of-thumb indicating that the tree is well-formed \cite{kamelfaloutsos94,theodoridissellis96}.

[To add: Hilbert curve description, image.]
Each inner node $N$ stores both its MBR as normal, but also its maximum Hilbert value: literally the furthest point on the Hilbert curve any child of $N$ touches.
This is akin to a B-tree node storing its maximum key value.
Lookup on a Hilbert tree behaves exactly as if it were an R-tree, but insertion, deletion, node-splitting, and node-merging behave exactly as if it were a B-tree, the leaves strictly ordered by their position on the Hilbert tree.
This was the new optimal design in its introduction, beating even the \rstar-tree in its initial experiments.
However, the Hilbert tree has a fixed curve---the curve cannot grow without essentially rebuilding the whole tree---and this limitation is not shared by the ``more'' dynamic R* tree \cite{beckmannseeger09}.

[How does this ordering improve performance?
Research in this area is ongoing, in particular determining \emph{which} Hilbert curve to use \cite{haverkortwalderveen11}.]

A key advantage implied by the B-tree-like behavior of the Hilbert tree is that node-splitting and merging is capable of being deferred just like in a B-tree.
This allows it to have much better space utilization \cite{kamelfaloutsos94}.
This better space utilization itself also improves performance, and harnessing that idea separately from space-filling curves is our third and final perspective.

[We have implicitly focused on imposing an ordering to create a well-formed tree, but this idea extends to spatial join as well \cite{jacoxsamet07}.]

\paragraph{Keeping the Tree Small}
In addition to runtime, one may want to improve storage utilization.
The first paper to focus on this for R-trees was \cite{huanglinlin01}.
Even in our storage-rich world, however, there are also time-based benefits for keeping an R-tree compact.
All other things being equal, an R-tree that manages to have fewer nodes immediately means queries have fewer nodes to check.
But, even in B-trees, there is a natural tradeoff between having cheap dynamic operations---insert and delete---and the time savings by having a more compact structure \cite{...}.
In \cite{huanglinlin01} they extend Guttman's tree to Compact R-Trees with the following simple rule: if an entry is being inserted into a node and that node will split, instead put that entry into a \emph{sibling} to our almost-splitting node.
Their experiments showed that it was competitive time-wise with Guttman's R-tree, but they did not compare it with the \rstar-tree.
And they achieved almost \%100 space utilization, in contrast to the \%70 of ``normal'' R-trees.

This idea has similarities to the branch grafting of \cite{schrekchen00} mentioned earlier.
Both branch grafting and Compact R-trees are mentioned in \cite{zhanglucheng06}, which is a recent paper on storage utilization in R-trees and contains a good summary of the history.
The main result presented there is a ``global'' version of the more local optimizations introduced in \cite{huanglinlin01}.

The main goal of this work is to improve storage usage.
In doing so, there are also real benefits to the performance of query execution.
[Does overlap really reduce?
I swear someone mentioned clustering: maybe in a bulk-loading paper?
These grafting techniques really work to improve tree shape as well, presenting opportunities to make ``little adjustments'' over the execution.
Not dissimilary from R*.]

