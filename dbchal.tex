As we have toured the classic enhancements and improvements to R-tree performance, we have encountered experimental data hinting towards when and how R-trees perform well and why.
In this section we discuss the papers that took that information and develop ways to understand R-tree performance.

\subsection{Cost model}
We hope to answer a very simple question: given a query, can we estimate how long that query will take?
The efficacy of an R-tree query is usually measured by the number of expected page accesses \cite{tocite}.
It is important to note that with spatial data, it is not unusual for computation time, not IO, to be the bottleneck \cite{tocite}.
Indeed, we assume our actual data is simple, perhaps axis-aligned rectangles themselves---otherwise taking into account the geometric complexity of our actual data further complicates things \cite{aboulnaganaughton00}.
[Add a sentence about CPU cost as a nontrivial factor in cost model.]
However, IO is the one universal non-trivial source of expense, so we focus on that.

So we hope to estimate how many page accesses an R-tree needs to satisfy a query for a given data set.
Whereas predicting B-tree performance is relatively straightforward \cite{tocite}, recall how R-tree performance varies greatly on how ``well'' it is organized.
Thus, it seems that predicting a query's cost is not just a function of the dataset, but also the structure of the R-tree built on it.
Indeed, the initial cost models for R-trees were index-dependent \cite{tocite}.
% \cite{see:list:in:theodoridisstefanakissellis}.
Moreover, those models largely assumed \emph{uniformly distributed} input---quite an assumption, especially for spatial data \cite{tocite}.

Fortunately, our understanding has advanced to the point where index-independent, cost-models are available for non-uniform data.
They are still parametrized by the spatial query, of course, but we are able to assume an arbitrary ``good'' R-tree.
One of the first was by Theodoridis et. al. \cite{theodoridissellis96,theodoridisstefanakissellis00}, for \emph{range} and \emph{join} queries.
Their cost model was only a function of the number of elements in the dataset and the spatial density.
Their inducing a \emph{density surface}, like a histogram mapping partitions of the space to the local density, was the key advance to extending their cost model beyond uniform input.
An alternative approach was fractal dimensionality, a concept introduced by \cite{tocite}.
It was extended to [stuff] by \cite{tocite}.

Our previous metrics assumed, or hoped for, some a priori knowledge about the data.
Failing that, we could attempt to estimate the properties of the data, which of course implies a means of maintaining that estimation.
This data is often applied for query optimization \cite{chaudhuri98}, and the histogram is one of the main tools for that \cite{poosalahaasioannidisshekita96}.

[Histograms maintained to improve R-tree performance]

R-trees can be used \emph{as} spatial histograms \cite{achakeevseeger12}, and serves to estimate a cost of a \emph{range} query.
Their spatial histogram is to partition the data rectangles $r_1,\ldots,r_N$ into buckets, each bucket maintaining the average $x,y$ lengths of its constituents $r_i,\ldots,r_j$ and the bucket's density.
Implied by its having a density, the bucket induces an MBR over its rectangles $r_i,\ldots,r_j$.
In in their paper \cite{achakeevseeger12}, the authors decide on an equi-depth histogram.
This is complicated by the fact that computing their optimal partition is $\NP$-hard \cite{muthukrishnanpoosalasuel99}.
Their design, guided by the cost-model of \cite{theodoridissellis96}, was thus computed with heuristics; the details are in \cite{achakeevseeger12a}.
 %
There also exist histograms for the purpose of estimating, say, a join \cite{aboulnaganaughton00} [other papers exist].
 %Also takes into account other stuff.
 %Some experimental models: \cite{aboulnaganaughton00}, \cite{anyangsivasubramaniam01}, \cite{achakeevseeger12,achakeevseeger12a} has good datasets discussion, see for ``data model''.

\subsection{Optimality R-tree}
It is important to mention that there exists an \emph{optimal} R-tree \cite{argeberghaverkortyi04}, called the PR-tree (Priority R-tree).
For our purposes, we have three core points of discussion about this structure.
1) What does it mean to be an optimal R-tree?
2) How does its behavior reflect the techniques and ideas from section~\ref{sec:impchal}?
3) What can this tell us about future development of R-trees?
We discuss each point in turn.
Points 1) and 3) are especially well-discussed in \cite{yi12}, which is a thorough summary of the results of the main paper, \cite{argeberghaverkortyi04}, and we direct the interested reader there.

The optimal R-tree is only built on static data, and answers range queries in optimal time.
(Note though that the PR-tree does have a fast insert---but it's not promised to maintain optimality---and a slow insert which maintains its optimality \cite{argeberghaverkortyi04}.)
From previous work \cite{kanthsingh99} it was known that a range query had a necessarily high upper bound in terms of node-accesses.
Formally, we have the [theorem].
Thus, if a structure could match that upper bound, that structure is optimal.
The optimal R-tree effectively matches that bound with a time [time] \cite{yi12}.
The extra $T/B$ term is obviously optimal cost for outputting the results, so the slight difference between [term1,term2] is inconsequential.
It is also important to note this does \emph{not} reflect a universal lower bound for range queries: there are objects other than R-trees [see yi12 citation 3].
Practically speaking, the optimal R-tree is competitive with its heuristic counterparts---moreover, it obviously does not perform poorly on any dataset.
In contrast, one can construct bad datasets for \rstar-trees and Hilbert R-trees.

Curiously, the behavior of this R-tree does not reflect our heuristic intuitions from section~\ref{sec:impchal}.
A nice description of the Priority R-tree's design can be found in \cite{thebook}, but we provide a brief one here as well.
The construction builds from prior results showing that it is possible to build an optimal-range-query \emph{KD-tree} \cite{agarwalberggudmundssonhammarhaverkort01}.
A KD-tree \cite{bentley75}, of which there are many varieties \cite{gaedegunther98}, is a structure similar to R-trees in that a node represents a small part of the total space, and the node's children represent a further refinement of that space.
It is for this general category of structures the range upper bound was determined.
Thus, the general strategy for the PR-tree can be loosely understood as a reduction to and from this KD-tree.
This is not to suggest the construction is trivial! Rather, it is to show the surprising fact that this does explicitly, nor seem to implicitly, use the heuristics developed for R-trees.

Does this PR-tree tell us anything about the future of R-trees?
It would be interesting to see if we can reconcile its optimality with our design intuitions.
The summary by Yi \cite{yi12} lists some immediate questions PR-trees: can it be extended with a fast, optimality-maintaining insert, thereby making it dynamic?
Does there exist an R-tree optimal for other queries (particularly nearest-neighbor)?
Lastly, the upper-bound theorem fails in the degenerate case when the query is a point---there we return to the notion of stabbing number.
There is a recent advance related to that last front: recall \cite{bergkhosraviverdonschotweele11} saying that computing the minimal stabbing-point MBR set covering a set of points is $\NP$-hard.

For general R-trees, it is intriguing that optimality in a setting was obtained by appealing to a different, but related, data structure.
Perhaps more of these sort of reductions can lead to further results.
