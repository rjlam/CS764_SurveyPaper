As we have toured the classic enhancements and improvements to R-tree performance, we have encountered ideas hinting towards when and how R-trees perform well and why.
In this section we discuss the papers that took that information and developed ways to understand R-tree performance.

\subsection{Cost model}
We hope to answer a very simple question: given a query, can we estimate how long that query will take?
The goal is to estimate some "cost" (usually the number of disk accesses) needed to compute a range query over a spatial dataset.
Some of the initial cost models were index-dependent---that is, they could only give an estimate knowing what the shape of the given R-tree was \cite{theodoridisstefanakissellis00} (see section 3).
These same models also assumed a uniform input distribution.
We assume our actual data is simple, perhaps axis-aligned rectangles themselves---otherwise taking into account the geometric complexity of our actual data further complicates things \cite{aboulnaganaughton00}.
The cost of CPU time can be nontrivial, especially during joins, simply because geometric queries can be substantially more complicated than, say, an equijoin \cite{theodoridisstefanakissellis00}.
However, that is not always the case, so, as I/O is the one universal non-trivial source of expense we focus on that.

We will present three index-independent cost models: density surface, fractal dimension, and min-skew partitioning.
A deeper discussion of these and more can be found in the book on R-trees \cite{thebook}.
In our attempt to classify the techniques and motivation, we point to an early paper on this topic \cite{acharyapoosalaramaswamy99}, which mentions the classification of histogram-based, sampling-based, and parametric-based cost models. This structure may guide one's understanding of the different models.

The first model we highlight is by Theodoridis et. al. \cite{theodoridissellis96,theodoridisstefanakissellis00} with their density surface approach.
This paper was an important milestone because it extricated R-tree cost models from both uniformity assumption and index-dependence.
The basic idea is to use the density (that is, how much space actually has data in it versus no data) to estimate how expensive a range query will be.
They move beyond the uniformity assumption basically through a histogram: their density surface maps partitions of the space to a value indicating their density.

Another attempt to eliminate the uniform assumption was to use fractal dimension \cite{faloutsoskamel94}. This is restricted only to point data, and is a parametric cost model.
Motivated by the application of Zipf to model nonuniform data distributions on regular indexes \cite{ioannidischristodoulakis91}, the authors started from the fact that a fractal both has an infinite number of points, but is highly non-uniform in their distribution.
Moreover, a fractal is self-describing: a small number of points dictates the global structure.
Thus, they work towards a fractal ``description'' of a given point dataset, and from there have a model for estimating range queries.

Lastly, we mention the work done on min-skew partitioning.
This is another histogram-like method, with the idea of grouping data into buckets such that the \emph{variance} of the density within each bucket is low \cite{acharyapoosalaramaswamy99}.
It is important to note that the perspective of the authors is in estimating the \emph{size of the output} of a range query, and \emph{not} the number of page accesses involved.
Their motivation is more for estimating facts about the spatial data, not the performance of a particular index over it.
In this sense their goals overlap with ours, but is not precisely about R-trees.

Indeed, this sort of overlap exists quite a bit in the field of ``spatial histograms''.
R-trees, as used in \cite{acharyapoosalaramaswamy99}, can be considered something like a histogram over the spatial data itself.
This viewpoint has continued, for instance by Achakeev and Seeger \cite{achakeevseeger12}.
Their spatial histogram partitions the data rectangles $r_1,\ldots,r_N$ into buckets, each bucket maintaining the average $x,y$ lengths of its constituents $r_i,\ldots,r_j$ and the bucket's density.
That they use R-trees as a heuristic for spatial histograms may be because computing their optimal partition is $\NP$-hard \cite{muthukrishnanpoosalasuel99}.
Their design, guided by the cost-model of \cite{theodoridissellis96}, was thus computed with heuristics; the details are in \cite{achakeevseeger12a}.
There is a brief but illuminating, and quite up-to-date, discussion of spatial histograms in a recent book on spatial access methods \cite{mamoulis11}.

Some final notes:
There are also many other takes on understanding R-tree cost models.
We have focused solely on the range query, but people have considered others, such as join \cite{thebook,aboulnaganaughton00}.
Due to our limited understanding of spatial data, a variety of real-world and generated data models have been considered, which are well worth investigating for understanding R-trees \cite{aboulnaganaughton00, anyangsivasubramaniam01, achakeevseeger12,achakeevseeger12a}.
If nothing else, there are clearly many perspectives one can take to measuring the performance and behavior of an R-tree.

\subsection{Optimal R-tree}
It is important to mention that there exists an \emph{optimal} R-tree \cite{argeberghaverkortyi04}, called the PR-tree (Priority R-tree).
For our purposes, we have three core points of discussion about this structure.
1) What does it mean to be an optimal R-tree?
2) How does its behavior reflect the techniques and ideas from section~\ref{sec:impchal}?
3) What can this tell us about future development of R-trees?
We discuss each point in turn.
Points 1) and 3) are especially well-discussed in \cite{yi12}, which is a thorough summary of the results of the main paper, \cite{argeberghaverkortyi04}, and we direct the interested reader there.

The optimal R-tree is only built on static data, and answers range queries in optimal time.
(Note though that the PR-tree does have a fast insert---but it's not promised to maintain optimality---and a slow insert which maintains its optimality \cite{argeberghaverkortyi04}.)
From previous work \cite{kanthsingh99} it was known that a range query had a necessarily high upper bound in terms of node-accesses.
Thus, if a structure could match that upper bound, that structure is optimal.
The optimal R-tree effectively matches that bound \cite{yi12}.
It is also important to note this does \emph{not} reflect a universal lower bound for range queries: there are objects other than R-trees \cite{argesamoladasvitter99}.
Practically speaking, the optimal R-tree is competitive with its heuristic counterparts---moreover, it obviously does not perform poorly on any dataset.
In contrast, one can construct bad datasets for \rstar-trees and Hilbert R-trees.

Curiously, the behavior of this R-tree does not reflect our heuristic intuitions from section~\ref{sec:impchal}.
A nice description of the Priority R-tree's design can be found in \cite{thebook}, but we provide a brief one here as well.
The construction builds from prior results showing that it is possible to build an optimal-range-query \emph{KD-tree} \cite{agarwalberggudmundssonhammarhaverkort01}.
A KD-tree \cite{bentley75}, of which there are many varieties \cite{gaedegunther98}, is a structure similar to R-trees in that a node represents a small part of the total space, and the node's children represent a further refinement of that space.
It is for this general category of structures the range upper bound was determined.
Thus, the general strategy for the PR-tree can be loosely understood as a reduction to and from this KD-tree.
This is not to suggest the construction is trivial! Rather, it is to show the surprising fact that this does explicitly, nor seem to implicitly, use the heuristics developed for R-trees.

Does this PR-tree tell us anything about the future of R-trees?
It would be interesting to see if we can reconcile its optimality with our design intuitions.
The summary by Yi \cite{yi12} lists some open questions about PR-trees: can it be extended with a fast, optimality-maintaining insert, thereby making it dynamic?
Does there exist an R-tree optimal for other queries (particularly nearest-neighbor)?
Lastly, the upper-bound theorem fails in the degenerate case when the query is a point---there we return to the notion of stabbing number: can we obtain a better bound?
There is a recent advance related to that last front: recall \cite{bergkhosraviverdonschotweele11} saying that computing the minimal stabbing-point MBR set covering a set of points is $\NP$-hard.

For general R-trees, it is intriguing that optimality in a setting was obtained by appealing to a different, but related, data structure.
Perhaps more of these sort of reductions can lead to further results.
